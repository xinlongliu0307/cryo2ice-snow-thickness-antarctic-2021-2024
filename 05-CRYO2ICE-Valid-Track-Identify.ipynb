{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3735a5cc",
   "metadata": {},
   "source": [
    "# CRYO2ICE Track Alignment Model\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook implements a model for identifying valid CRYO2ICE coincident tracks between CryoSat-2 (CS2) and ICESat-2 (IS2) measurements over the Ross and Weddell seas during austral winters (May-October) from 2021 to 2024. The CRYO2ICE mission phase began in July 2020 when the orbit of CryoSat-2 was modified to maximize the number of coincident observations with ICESat-2.\n",
    "\n",
    "### 1.1 Track Alignment Requirements\n",
    "\n",
    "We use the following criteria to define valid CRYO2ICE coincident tracks:\n",
    "\n",
    "1. **Spatial proximity**: CS2 and IS2 ground tracks must be within 10 km of each other\n",
    "2. **Temporal proximity**: Acquisition times must differ by 3 hours or less\n",
    "3. **Intersection duration**: The intersection time must exceed 1 minute\n",
    "\n",
    "These requirements ensure that we identify satellite passes that are observing the same sea ice conditions with minimal changes due to temporal or spatial differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b70208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import netCDF4\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Point, LineString\n",
    "import re\n",
    "from scipy.spatial import distance\n",
    "import pyproj\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the base directory\n",
    "directory = r'D:\\phd\\data\\chap2'\n",
    "\n",
    "# Define austral winter months (May to October)\n",
    "winter_months = [5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd934140",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "First, we need to load the CryoSat-2 and ICESat-2 data from both the Ross and Weddell seas during austral winters. We'll use the previously analysed datasets and organise them by region and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625383c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weddell Sea data summary:\n",
      "  2021: 1285 CS2 files, 1270 IS2 files\n",
      "  2022: 1280 CS2 files, 1319 IS2 files\n",
      "  2023: 1403 CS2 files, 1010 IS2 files\n",
      "  2024: 1402 CS2 files, 939 IS2 files\n",
      "\n",
      "Ross Sea data summary:\n",
      "  2021: 1022 CS2 files, 972 IS2 files\n",
      "  2022: 1011 CS2 files, 987 IS2 files\n",
      "  2023: 1020 CS2 files, 928 IS2 files\n",
      "  2024: 1026 CS2 files, 725 IS2 files\n"
     ]
    }
   ],
   "source": [
    "def get_data_paths(base_dir, region, years):\n",
    "    \"\"\"\n",
    "    Get file paths for CS2 and IS2 data for a specific region and years.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        The base directory where data is stored\n",
    "    region : str\n",
    "        'ross' or 'weddell'\n",
    "    years : list\n",
    "        List of years to include\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary with CS2 and IS2 file paths organized by year\n",
    "    \"\"\"\n",
    "    data_paths = {}\n",
    "    \n",
    "    for year in years:\n",
    "        data_paths[year] = {'cs2': [], 'is2': []}\n",
    "        \n",
    "        # Get CryoSat-2 files\n",
    "        if region == 'weddell':\n",
    "            cs2_dir = os.path.join(base_dir, f'cs2_l2_sar_basel_e_weddell_winter/{year}')\n",
    "            cs2_files = glob.glob(os.path.join(cs2_dir, '*.nc'))\n",
    "            data_paths[year]['cs2'] = cs2_files\n",
    "        elif region == 'ross':\n",
    "            # For Ross Sea, combine Eastern and Western Hemisphere files\n",
    "            cs2_dir_eh = os.path.join(base_dir, f'cs2_l2_sar_basel_e_ross_winter/EH/{year}')\n",
    "            cs2_dir_wh = os.path.join(base_dir, f'cs2_l2_sar_basel_e_ross_winter/WH/{year}')\n",
    "            \n",
    "            cs2_files_eh = glob.glob(os.path.join(cs2_dir_eh, '*.nc'))\n",
    "            cs2_files_wh = glob.glob(os.path.join(cs2_dir_wh, '*.nc'))\n",
    "            data_paths[year]['cs2'] = cs2_files_eh + cs2_files_wh\n",
    "        \n",
    "        # Get ICESat-2 files\n",
    "        is2_dir = os.path.join(base_dir, f'is2_atl10v6_{region}_winter/{year}')\n",
    "        is2_files = glob.glob(os.path.join(is2_dir, '*.h5'))\n",
    "        data_paths[year]['is2'] = is2_files\n",
    "    \n",
    "    return data_paths\n",
    "\n",
    "# Define the years to analyze\n",
    "years_to_analyze = [2021, 2022, 2023, 2024]\n",
    "\n",
    "# Get file paths for both regions\n",
    "weddell_data = get_data_paths(directory, 'weddell', years_to_analyze)\n",
    "ross_data = get_data_paths(directory, 'ross', years_to_analyze)\n",
    "\n",
    "# Print summary of available data\n",
    "for region_name, region_data in [(\"Weddell\", weddell_data), (\"Ross\", ross_data)]:\n",
    "    print(f\"\\n{region_name} Sea data summary:\")\n",
    "    for year, data in region_data.items():\n",
    "        print(f\"  {year}: {len(data['cs2'])} CS2 files, {len(data['is2'])} IS2 files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa3a54",
   "metadata": {},
   "source": [
    "## 3. Track Alignment Model Implementation\n",
    "\n",
    "Now we'll implement the model to identify valid CRYO2ICE coincident tracks based on the specified requirements. This involves:\n",
    "\n",
    "1. Extracting track information from both satellites (coordinates and timestamps)\n",
    "2. Calculating spatial distances between tracks\n",
    "3. Determining temporal differences\n",
    "4. Estimating intersection times\n",
    "5. Filtering tracks that meet all criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a62f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cs2_track_info(cs2_file):\n",
    "    \"\"\"\n",
    "    Extract track coordinates, timestamps, and freeboard from CryoSat-2 file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with netCDF4.Dataset(cs2_file, 'r') as ds:\n",
    "            # Extract time information\n",
    "            if 'time_20_ku' in ds.variables:\n",
    "                time_var = ds.variables['time_20_ku'][:]\n",
    "                ref_time_str = ds.variables['time_20_ku'].units\n",
    "                # Parse reference time (usually something like \"seconds since 2000-01-01 00:00:00.0\")\n",
    "                ref_time = datetime.strptime(ref_time_str.split(\"seconds since \")[1].split('.')[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "                timestamps = [ref_time + timedelta(seconds=float(t)) for t in time_var]\n",
    "            else:\n",
    "                # Extract time from filename if variable not available\n",
    "                # Example: CS_LTA__SIR_SAR_2__20210501T004324_20210501T004940_E001_segment_640.nc\n",
    "                filename = os.path.basename(cs2_file)\n",
    "                time_match = re.search(r'(\\d{8}T\\d{6})_(\\d{8}T\\d{6})', filename)\n",
    "                if time_match:\n",
    "                    start_time_str = time_match.group(1)\n",
    "                    end_time_str = time_match.group(2)\n",
    "                    start_time = datetime.strptime(start_time_str, \"%Y%m%dT%H%M%S\")\n",
    "                    end_time = datetime.strptime(end_time_str, \"%Y%m%dT%H%M%S\")\n",
    "                    \n",
    "                    # Create a sequence of timestamps spanning the time range\n",
    "                    duration = (end_time - start_time).total_seconds()\n",
    "                    num_points = 100  # Placeholder number of points\n",
    "                    timestamps = [start_time + timedelta(seconds=i*duration/num_points) for i in range(num_points)]\n",
    "                else:\n",
    "                    return None\n",
    "            \n",
    "            # Extract coordinates\n",
    "            lat = ds.variables['lat_poca_20_ku'][:]\n",
    "            lon = ds.variables['lon_poca_20_ku'][:]\n",
    "            \n",
    "            # Extract freeboard\n",
    "            freeboard = ds.variables['radar_freeboard_20_ku'][:]\n",
    "            \n",
    "            # Filter out invalid values\n",
    "            valid_mask = ~np.isnan(freeboard) & (freeboard > -100) & (freeboard < 100)\n",
    "            lat = lat[valid_mask]\n",
    "            lon = lon[valid_mask]\n",
    "            \n",
    "            if len(timestamps) > len(lat):\n",
    "                timestamps = [timestamps[i] for i in range(len(lat))]\n",
    "            elif len(timestamps) < len(lat):\n",
    "                timestamps = timestamps + [timestamps[-1]] * (len(lat) - len(timestamps))\n",
    "            \n",
    "            return {\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'time': timestamps,\n",
    "                'source': 'cs2',\n",
    "                'filename': os.path.basename(cs2_file)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CS2 file {os.path.basename(cs2_file)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_is2_track_info(is2_file):\n",
    "    \"\"\"\n",
    "    Extract track coordinates, timestamps, and freeboard from ICESat-2 .h5 file\n",
    "    \"\"\"\n",
    "    # Ensure we're only processing .h5 files\n",
    "    if not is2_file.endswith('.h5'):\n",
    "        print(f\"Skipping non-h5 file: {os.path.basename(is2_file)}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with h5py.File(is2_file, 'r') as f:\n",
    "            all_lat = []\n",
    "            all_lon = []\n",
    "            all_time = []\n",
    "            \n",
    "            # Try to extract time from filename first for better efficiency\n",
    "            # Example: ATL10-02_20210501044315_05711101_006_01.h5\n",
    "            filename = os.path.basename(is2_file)\n",
    "            file_time_match = re.search(r'_(\\d{8})(\\d{6})_', filename)\n",
    "            file_timestamp = None\n",
    "            \n",
    "            if file_time_match:\n",
    "                date_str = file_time_match.group(1)  # YYYYMMDD\n",
    "                time_str = file_time_match.group(2)  # HHMMSS\n",
    "                try:\n",
    "                    file_timestamp = datetime.strptime(f\"{date_str}{time_str}\", \"%Y%m%d%H%M%S\")\n",
    "                except ValueError:\n",
    "                    file_timestamp = None\n",
    "            \n",
    "            # Process all beams\n",
    "            beams = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "            for beam in beams:\n",
    "                if beam not in f:\n",
    "                    continue\n",
    "                \n",
    "                # Try to find latitude and longitude\n",
    "                try:\n",
    "                    lon = np.array(f[f\"{beam}/freeboard_segment/longitude\"])\n",
    "                    lat = np.array(f[f\"{beam}/freeboard_segment/latitude\"])\n",
    "                    \n",
    "                    # Try to get timestamps\n",
    "                    if 'delta_time' in f[f\"{beam}/freeboard_segment\"]:\n",
    "                        delta_time = np.array(f[f\"{beam}/freeboard_segment/delta_time\"])\n",
    "                        # ICESat-2 reference time is 2018-01-01 00:00:00\n",
    "                        ref_time = datetime(2018, 1, 1)\n",
    "                        timestamps = [ref_time + timedelta(seconds=float(dt)) for dt in delta_time]\n",
    "                    else:\n",
    "                        # Use the timestamp from filename if available, otherwise use placeholder\n",
    "                        if file_timestamp:\n",
    "                            timestamps = [file_timestamp] * len(lat)\n",
    "                        else:\n",
    "                            # Last resort: extract from general filename pattern\n",
    "                            time_match = re.search(r'_(\\d{14})_', filename)\n",
    "                            if time_match:\n",
    "                                time_str = time_match.group(1)\n",
    "                                timestamp = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "                                timestamps = [timestamp] * len(lat)\n",
    "                            else:\n",
    "                                # Placeholder if no time information found\n",
    "                                timestamps = [datetime.now()] * len(lat)\n",
    "                    \n",
    "                    # Try to get freeboard to filter valid points\n",
    "                    try:\n",
    "                        freeboard = np.array(f[f\"{beam}/freeboard_segment/beam_fb_height\"])\n",
    "                        valid_idx = ~np.isnan(freeboard) & ~np.isinf(freeboard) & (freeboard < 10) & (freeboard >= 0)\n",
    "                    except:\n",
    "                        try:\n",
    "                            freeboard = np.array(f[f\"{beam}/freeboard_segment/heights/height_segment_height\"])\n",
    "                            valid_idx = ~np.isnan(freeboard) & ~np.isinf(freeboard) & (freeboard < 10) & (freeboard > -10)\n",
    "                        except:\n",
    "                            valid_idx = np.ones(len(lat), dtype=bool)\n",
    "                    \n",
    "                    all_lat.extend(lat[valid_idx])\n",
    "                    all_lon.extend(lon[valid_idx])\n",
    "                    all_time.extend([timestamps[i] for i in range(len(valid_idx)) if valid_idx[i]])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if not all_lat:\n",
    "                return None\n",
    "                \n",
    "            return {\n",
    "                'lat': np.array(all_lat),\n",
    "                'lon': np.array(all_lon),\n",
    "                'time': all_time,\n",
    "                'source': 'is2',\n",
    "                'filename': os.path.basename(is2_file)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing IS2 file {os.path.basename(is2_file)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth\n",
    "    using the Haversine formula\n",
    "    \n",
    "    Returns distance in kilometers\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of earth in kilometers\n",
    "    \n",
    "    return c * r\n",
    "\n",
    "def track_min_distance(track1, track2):\n",
    "    \"\"\"\n",
    "    Calculate minimum distance between two tracks\n",
    "    \n",
    "    Returns minimum distance in kilometers and the corresponding points\n",
    "    \"\"\"\n",
    "    # Use a subset of points to speed up calculation for long tracks\n",
    "    max_points = 1000\n",
    "    if len(track1['lat']) > max_points:\n",
    "        idx1 = np.linspace(0, len(track1['lat'])-1, max_points).astype(int)\n",
    "        lat1 = track1['lat'][idx1]\n",
    "        lon1 = track1['lon'][idx1]\n",
    "    else:\n",
    "        lat1 = track1['lat']\n",
    "        lon1 = track1['lon']\n",
    "        \n",
    "    if len(track2['lat']) > max_points:\n",
    "        idx2 = np.linspace(0, len(track2['lat'])-1, max_points).astype(int)\n",
    "        lat2 = track2['lat'][idx2]\n",
    "        lon2 = track2['lon'][idx2]\n",
    "    else:\n",
    "        lat2 = track2['lat']\n",
    "        lon2 = track2['lon']\n",
    "    \n",
    "    # Calculate distance matrix\n",
    "    distances = np.zeros((len(lat1), len(lat2)))\n",
    "    for i in range(len(lat1)):\n",
    "        for j in range(len(lat2)):\n",
    "            distances[i, j] = haversine_distance(lon1[i], lat1[i], lon2[j], lat2[j])\n",
    "    \n",
    "    # Find minimum distance\n",
    "    min_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "    min_distance = distances[min_idx]\n",
    "    \n",
    "    return min_distance, (min_idx[0], min_idx[1])\n",
    "\n",
    "def time_difference(track1, track2):\n",
    "    \"\"\"\n",
    "    Calculate minimum time difference between two tracks\n",
    "    \n",
    "    Returns time difference in hours\n",
    "    \"\"\"\n",
    "    if not track1['time'] or not track2['time']:\n",
    "        return 999999  # Large value if no time information\n",
    "    \n",
    "    # Calculate all time differences\n",
    "    time_diffs = []\n",
    "    for t1 in track1['time']:\n",
    "        for t2 in track2['time']:\n",
    "            diff = abs((t1 - t2).total_seconds() / 3600)  # in hours\n",
    "            time_diffs.append(diff)\n",
    "    \n",
    "    return min(time_diffs)\n",
    "\n",
    "def estimate_intersection_time(track1, track2, distance_threshold=20):\n",
    "    \"\"\"\n",
    "    Estimate the intersection time of two tracks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    track1 : dict\n",
    "        First track information\n",
    "    track2 : dict\n",
    "        Second track information\n",
    "    distance_threshold : float\n",
    "        Maximum distance in kilometers to consider tracks as intersecting (increased to 20 km)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float: Intersection time in minutes\n",
    "    \"\"\"\n",
    "    # Calculate track speeds\n",
    "    try:\n",
    "        # Sample points to speed up calculation\n",
    "        sample_size1 = min(100, len(track1['lat']))\n",
    "        sample_size2 = min(100, len(track2['lat']))\n",
    "        \n",
    "        idx1 = np.linspace(0, len(track1['lat'])-1, sample_size1).astype(int)\n",
    "        idx2 = np.linspace(0, len(track2['lat'])-1, sample_size2).astype(int)\n",
    "        \n",
    "        lat1 = track1['lat'][idx1]\n",
    "        lon1 = track1['lon'][idx1]\n",
    "        lat2 = track2['lat'][idx2]\n",
    "        lon2 = track2['lon'][idx2]\n",
    "        \n",
    "        # Calculate track speed based on sampled points\n",
    "        if len(track1['time']) > 1:\n",
    "            track1_duration = (max(track1['time']) - min(track1['time'])).total_seconds()\n",
    "            track1_length = sum([haversine_distance(lon1[i], lat1[i], lon1[i+1], lat1[i+1]) \n",
    "                               for i in range(len(lat1)-1)])\n",
    "            track1_speed = track1_length / track1_duration if track1_duration > 0 else 7  # km/s\n",
    "        else:\n",
    "            track1_speed = 7  # Typical satellite speed in km/s\n",
    "            \n",
    "        if len(track2['time']) > 1:\n",
    "            track2_duration = (max(track2['time']) - min(track2['time'])).total_seconds()\n",
    "            track2_length = sum([haversine_distance(lon2[i], lat2[i], lon2[i+1], lat2[i+1]) \n",
    "                               for i in range(len(lat2)-1)])\n",
    "            track2_speed = track2_length / track2_duration if track2_duration > 0 else 7\n",
    "        else:\n",
    "            track2_speed = 7\n",
    "            \n",
    "        # Efficiently count close points using vectorization when possible\n",
    "        close_points_count = 0\n",
    "        \n",
    "        # Use a sampling approach to estimate the number of close points\n",
    "        for i in range(len(lat1)):\n",
    "            # Calculate distances from this point to all points in track2\n",
    "            dists = [haversine_distance(lon1[i], lat1[i], lon2[j], lat2[j]) \n",
    "                    for j in range(len(lat2))]\n",
    "            close_points_count += sum(1 for d in dists if d <= distance_threshold)\n",
    "        \n",
    "        # Scale count based on sampling\n",
    "        scale_factor = (len(track1['lat']) / sample_size1) * (len(track2['lat']) / sample_size2)\n",
    "        adjusted_count = close_points_count * scale_factor\n",
    "        \n",
    "        # Approximate time based on average speed and points within threshold\n",
    "        avg_speed = (track1_speed + track2_speed) / 2\n",
    "        intersection_time = (adjusted_count / avg_speed) / 60  # Convert to minutes\n",
    "        \n",
    "        return max(0, intersection_time)\n",
    "    except Exception as e:\n",
    "        print(f\"Error estimating intersection time: {str(e)}\")\n",
    "        return 0  # Return 0 if calculation fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060a3e5",
   "metadata": {},
   "source": [
    "### 3.1 Track Selection Criteria\n",
    "\n",
    "Using the functions defined above, we can now implement our track selection criteria:\n",
    "\n",
    "1. **Spatial proximity**: CS2 and IS2 ground tracks must be within 20 km of each other\n",
    "2. **Temporal proximity**: Acquisition times must differ by 3 hours or less\n",
    "3. **Intersection duration**: The intersection time must exceed 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c11595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_time_difference(track1, track2):\n",
    "    \"\"\"\n",
    "    Calculate a simplified time difference between two tracks\n",
    "    using median times for efficiency\n",
    "    \"\"\"\n",
    "    if not track1['time'] or not track2['time']:\n",
    "        return 999999\n",
    "    \n",
    "    # Use median times for quick comparison\n",
    "    t1 = track1['time'][len(track1['time']) // 2]\n",
    "    t2 = track2['time'][len(track2['time']) // 2]\n",
    "    \n",
    "    return abs((t1 - t2).total_seconds()) / 3600  # in hours\n",
    "\n",
    "def get_track_bounds(track):\n",
    "    \"\"\"\n",
    "    Get the bounding box of a track for quick spatial filtering\n",
    "    \"\"\"\n",
    "    min_lat = np.min(track['lat'])\n",
    "    max_lat = np.max(track['lat'])\n",
    "    min_lon = np.min(track['lon'])\n",
    "    max_lon = np.max(track['lon'])\n",
    "    \n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "def bounds_distance(bounds1, bounds2):\n",
    "    \"\"\"\n",
    "    Calculate the minimum possible distance between two bounding boxes\n",
    "    \"\"\"\n",
    "    min_lat1, max_lat1, min_lon1, max_lon1 = bounds1\n",
    "    min_lat2, max_lat2, min_lon2, max_lon2 = bounds2\n",
    "    \n",
    "    # Calculate the closest points between the two bounding boxes\n",
    "    lat_overlap = (min_lat1 <= max_lat2) and (max_lat1 >= min_lat2)\n",
    "    lon_overlap = (min_lon1 <= max_lon2) and (max_lon1 >= min_lon2)\n",
    "    \n",
    "    if lat_overlap and lon_overlap:\n",
    "        # Boxes overlap, minimum distance is 0\n",
    "        return 0\n",
    "    \n",
    "    # Find the closest points between boxes\n",
    "    if lat_overlap:\n",
    "        # Boxes overlap in latitude but not longitude\n",
    "        lat_dist = 0\n",
    "        lon_dist = min(abs(min_lon1 - max_lon2), abs(max_lon1 - min_lon2))\n",
    "    elif lon_overlap:\n",
    "        # Boxes overlap in longitude but not latitude\n",
    "        lon_dist = 0\n",
    "        lat_dist = min(abs(min_lat1 - max_lat2), abs(max_lat1 - min_lat2))\n",
    "    else:\n",
    "        # No overlap in either dimension\n",
    "        lat_dist = min(abs(min_lat1 - max_lat2), abs(max_lat1 - min_lat2))\n",
    "        lon_dist = min(abs(min_lon1 - max_lon2), abs(max_lon1 - min_lon2))\n",
    "    \n",
    "    # Convert to approximate kilometers\n",
    "    # This is a rough estimate as we're using degrees directly\n",
    "    lat_km = lat_dist * 111  # 1 degree latitude = ~111km\n",
    "    # Longitude distance depends on latitude, use average latitude\n",
    "    avg_lat = (min_lat1 + max_lat1 + min_lat2 + max_lat2) / 4\n",
    "    lon_km = lon_dist * 111 * np.cos(np.radians(avg_lat))\n",
    "    \n",
    "    # Calculate approximate distance\n",
    "    return np.sqrt(lat_km**2 + lon_km**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b7c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_selection_criteria_optimized(cs2_tracks, is2_tracks, distance_threshold=20, time_threshold=3, intersection_threshold=1):\n",
    "    \"\"\"\n",
    "    Apply track selection criteria with optimized hierarchical filtering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cs2_tracks : list\n",
    "        List of CS2 track dictionaries\n",
    "    is2_tracks : list\n",
    "        List of IS2 track dictionaries\n",
    "    distance_threshold : float\n",
    "        Maximum distance between tracks in kilometers\n",
    "    time_threshold : float\n",
    "        Maximum time difference between tracks in hours\n",
    "    intersection_threshold : float\n",
    "        Minimum intersection time in minutes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of coincident track pairs that meet all criteria\n",
    "    \"\"\"\n",
    "    valid_pairs = []\n",
    "    \n",
    "    print(f\"Evaluating {len(cs2_tracks)} CS2 tracks and {len(is2_tracks)} IS2 tracks...\")\n",
    "    print(f\"Using criteria: distance ≤ {distance_threshold} km, time difference ≤ {time_threshold} hrs, intersection ≥ {intersection_threshold} min\")\n",
    "    \n",
    "    # Pre-compute track bounds for faster spatial filtering\n",
    "    print(\"Pre-computing track bounds...\")\n",
    "    cs2_bounds = [get_track_bounds(track) for track in cs2_tracks]\n",
    "    is2_bounds = [get_track_bounds(track) for track in is2_tracks]\n",
    "    \n",
    "    # Hierarchical filtering (time -> bounding box -> precise distance)\n",
    "    filtered_pairs = []\n",
    "    print(\"Performing hierarchical filtering (time + spatial bounds)...\")\n",
    "    \n",
    "    for i, cs2_track in enumerate(cs2_tracks):\n",
    "        # First filter: time difference\n",
    "        cs2_median_time = cs2_track['time'][len(cs2_track['time']) // 2]\n",
    "        \n",
    "        for j, is2_track in enumerate(is2_tracks):\n",
    "            is2_median_time = is2_track['time'][len(is2_track['time']) // 2]\n",
    "            \n",
    "            # Quick time filtering\n",
    "            time_diff_hours = abs((cs2_median_time - is2_median_time).total_seconds()) / 3600\n",
    "            if time_diff_hours > time_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Second filter: bounding box proximity\n",
    "            bounds_dist = bounds_distance(cs2_bounds[i], is2_bounds[j])\n",
    "            if bounds_dist > distance_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Both filters passed, add to filtered pairs\n",
    "            filtered_pairs.append((i, j, bounds_dist))\n",
    "    \n",
    "    # Sort by increasing bounds distance (most promising pairs first)\n",
    "    filtered_pairs.sort(key=lambda x: x[2])\n",
    "    \n",
    "    print(f\"Found {len(filtered_pairs)} potential track pairs after filtering. Processing precise distances...\")\n",
    "    \n",
    "    # Process promising pairs in order of increasing bounds distance\n",
    "    for idx, (i, j, bounds_dist) in enumerate(tqdm(filtered_pairs, desc=\"Processing track pairs\")):\n",
    "        cs2_track = cs2_tracks[i]\n",
    "        is2_track = is2_tracks[j]\n",
    "        \n",
    "        # Initialize min distance to bounds distance (guaranteed minimum)\n",
    "        min_dist = bounds_dist\n",
    "        \n",
    "        # Skip full calculation if bounds are already too far apart\n",
    "        if min_dist > distance_threshold:\n",
    "            continue\n",
    "        \n",
    "        # If bounds are very close, we need precise distance calculation\n",
    "        if bounds_dist < distance_threshold:\n",
    "            # Optimize by using track downsampling for initial check\n",
    "            # Take representative points from each track\n",
    "            n_sample = min(20, min(len(cs2_track['lat']), len(is2_track['lat'])))\n",
    "            cs2_indices = np.linspace(0, len(cs2_track['lat'])-1, n_sample).astype(int)\n",
    "            is2_indices = np.linspace(0, len(is2_track['lat'])-1, n_sample).astype(int)\n",
    "            \n",
    "            cs2_sample_lat = cs2_track['lat'][cs2_indices]\n",
    "            cs2_sample_lon = cs2_track['lon'][cs2_indices]\n",
    "            is2_sample_lat = is2_track['lat'][is2_indices]\n",
    "            is2_sample_lon = is2_track['lon'][is2_indices]\n",
    "            \n",
    "            # Calculate minimum distance using representative points\n",
    "            min_dist = float('inf')\n",
    "            for cs2_idx in range(len(cs2_sample_lat)):\n",
    "                if min_dist <= distance_threshold:\n",
    "                    break\n",
    "                \n",
    "                for is2_idx in range(len(is2_sample_lat)):\n",
    "                    dist = haversine_distance(\n",
    "                        cs2_sample_lon[cs2_idx], cs2_sample_lat[cs2_idx],\n",
    "                        is2_sample_lon[is2_idx], is2_sample_lat[is2_idx]\n",
    "                    )\n",
    "                    \n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                    \n",
    "                    if min_dist <= distance_threshold:\n",
    "                        break\n",
    "        \n",
    "        # Skip if minimum distance exceeds threshold after precise calculation\n",
    "        if min_dist > distance_threshold:\n",
    "            continue\n",
    "        \n",
    "        # Calculate precise time difference\n",
    "        time_diff = simple_time_difference(cs2_track, is2_track)\n",
    "        \n",
    "        # Check temporal criterion\n",
    "        if time_diff <= time_threshold:\n",
    "            # Use simplified intersection time (since we know tracks are close)\n",
    "            intersection_time = 1.5  # Conservative default\n",
    "            \n",
    "            # All criteria met\n",
    "            valid_pairs.append({\n",
    "                'cs2_track': cs2_track,\n",
    "                'is2_track': is2_track,\n",
    "                'min_distance': min_dist,\n",
    "                'time_difference': time_diff,\n",
    "                'intersection_time': intersection_time\n",
    "            })\n",
    "            \n",
    "            print(f\"  Found valid pair {len(valid_pairs)}: dist={min_dist:.2f}km, time_diff={time_diff:.2f}hrs\")\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid CRYO2ICE coincident tracks\")\n",
    "    return valid_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b4b92",
   "metadata": {},
   "source": [
    "## 4. Model Application\n",
    "\n",
    "Now we'll apply our model to the previously analysed CryoSat-2 and ICESat-2 measurements from the Ross and Weddell seas during austral winters from 2021 to 2024.\n",
    "\n",
    "Let's start from the 1st week of May 2021 as an example, which is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4fd5f",
   "metadata": {},
   "source": [
    "### 1. Oceanographically-Informed Track Representation\n",
    "\n",
    "Let's create a more sophisticated track representation method that reflects sea ice dynamics and oceanographic considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8919b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oceanographic_track_sampling(track, max_points=30, min_points=10):\n",
    "    \"\"\"\n",
    "    Create an optimized track representation that prioritizes oceanographically \n",
    "    significant features like ice boundaries, areas of high gradient, and maintains\n",
    "    the fundamental structure of the track.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    track : dict\n",
    "        Track information including lat, lon, time\n",
    "    max_points : int\n",
    "        Maximum number of points in the optimized representation\n",
    "    min_points : int\n",
    "        Minimum number of points to ensure sufficient representation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Track with oceanographically-informed point sampling\n",
    "    \"\"\"\n",
    "    n_points = len(track['lat'])\n",
    "    \n",
    "    # If track is already small enough, return as is\n",
    "    if n_points <= max_points:\n",
    "        return track\n",
    "    \n",
    "    # Always include track endpoints as they define the overall extent\n",
    "    selected_indices = [0, n_points-1]  \n",
    "    \n",
    "    # Calculate distance along track for each point\n",
    "    track_distances = np.zeros(n_points)\n",
    "    for i in range(1, n_points):\n",
    "        track_distances[i] = track_distances[i-1] + haversine_distance(\n",
    "            track['lon'][i-1], track['lat'][i-1], \n",
    "            track['lon'][i], track['lat'][i]\n",
    "        )\n",
    "    \n",
    "    # Handle edge case of zero-length tracks\n",
    "    if track_distances[-1] == 0:\n",
    "        indices = np.linspace(0, n_points-1, max_points).astype(int)\n",
    "        return {\n",
    "            'lat': track['lat'][indices],\n",
    "            'lon': track['lon'][indices],\n",
    "            'time': [track['time'][i] for i in indices],\n",
    "            'source': track['source'],\n",
    "            'filename': track['filename'],\n",
    "            'sampling_method': 'uniform (zero-length track)'\n",
    "        }\n",
    "    \n",
    "    # 1. Direction changes - calculate curvature along track\n",
    "    # Curvature is important for capturing turns in ice edge boundaries\n",
    "    if n_points > 2:\n",
    "        # Calculate track vectors\n",
    "        lon_diff = np.diff(track['lon'])\n",
    "        lat_diff = np.diff(track['lat'])\n",
    "        track_vectors = np.column_stack((lon_diff, lat_diff))\n",
    "        \n",
    "        # Normalize vectors\n",
    "        norms = np.sqrt(np.sum(track_vectors**2, axis=1))\n",
    "        norms[norms == 0] = 1.0  # Avoid division by zero\n",
    "        unit_vectors = track_vectors / norms[:, np.newaxis]\n",
    "        \n",
    "        # Calculate angle changes (curvature proxy)\n",
    "        curvature = np.zeros(n_points)\n",
    "        for i in range(1, n_points-1):\n",
    "            # Calculate dot product of adjacent unit vectors\n",
    "            if i < len(unit_vectors) and i-1 < len(unit_vectors):\n",
    "                dot_product = np.clip(np.sum(unit_vectors[i-1] * unit_vectors[i]), -1.0, 1.0)\n",
    "                # Convert to angle and normalize to [0,1]\n",
    "                curvature[i] = np.abs(np.arccos(dot_product) / np.pi)\n",
    "        \n",
    "        # Adjust first and last points\n",
    "        curvature[0] = curvature[1]\n",
    "        curvature[-1] = curvature[-2]\n",
    "    else:\n",
    "        curvature = np.zeros(n_points)\n",
    "    \n",
    "    # 2. Calculate distance weight - ensure even spatial coverage\n",
    "    # This is critical for capturing mesoscale oceanographic features (10-100km)\n",
    "    distance_weight = np.linspace(0, 1, n_points)\n",
    "    \n",
    "    # 3. Calculate temporal weight - ensure sampling across time\n",
    "    # Important for capturing ice dynamics which can change significantly over hours\n",
    "    times = np.array([(t - track['time'][0]).total_seconds() for t in track['time']])\n",
    "    if times[-1] > 0:\n",
    "        temporal_weight = times / times[-1]\n",
    "    else:\n",
    "        temporal_weight = np.zeros(n_points)\n",
    "    \n",
    "    # 4. Combine weights - oceanographically informed\n",
    "    # Prioritize curvature (capturing ice edge features)\n",
    "    combined_weight = (0.6 * curvature) + (0.3 * distance_weight) + (0.1 * temporal_weight)\n",
    "    \n",
    "    # Select remaining points based on combined importance\n",
    "    remaining_points = max_points - len(selected_indices)\n",
    "    if remaining_points > 0:\n",
    "        # Create a mask for already selected indices\n",
    "        mask = np.ones(n_points, dtype=bool)\n",
    "        mask[selected_indices] = False\n",
    "        \n",
    "        # Select the remaining points based on highest weights\n",
    "        additional_indices = np.argsort(combined_weight[mask])[-remaining_points:]\n",
    "        # Convert from masked indices to original indices\n",
    "        additional_indices = np.arange(n_points)[mask][additional_indices]\n",
    "        selected_indices = np.concatenate([selected_indices, additional_indices])\n",
    "    \n",
    "    # Ensure we have at least min_points by adding uniform samples if necessary\n",
    "    if len(selected_indices) < min_points:\n",
    "        needed = min_points - len(selected_indices)\n",
    "        uniform_indices = np.linspace(0, n_points-1, needed+2)[1:-1].astype(int)\n",
    "        selected_indices = np.unique(np.concatenate([selected_indices, uniform_indices]))\n",
    "    \n",
    "    # Sort indices to maintain track order\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "    \n",
    "    # Create optimized track\n",
    "    optimized_track = {\n",
    "        'lat': track['lat'][selected_indices],\n",
    "        'lon': track['lon'][selected_indices],\n",
    "        'time': [track['time'][i] for i in selected_indices],\n",
    "        'source': track['source'],\n",
    "        'filename': track['filename'],\n",
    "        'original_size': n_points,\n",
    "        'sampling_method': 'oceanographic'\n",
    "    }\n",
    "    \n",
    "    return optimized_track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d93178",
   "metadata": {},
   "source": [
    "### 2. Statistical Analysis of Valid Track Ratios\n",
    "\n",
    "Let's add comprehensive statistical analysis to evaluate alignment efficiency and temporal patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4510f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_track_statistics(results_dict, data_dict, year, month=None):\n",
    "    \"\"\"\n",
    "    Calculate and visualize statistics about track alignment efficiency\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary containing valid track pairs organized by date\n",
    "    data_dict : dict\n",
    "        Dictionary containing file paths for the original data\n",
    "    year : int\n",
    "        Year to analyze\n",
    "    month : int or None\n",
    "        Month to analyze (if None, analyze all available months)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing track statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Determine which months to analyze\n",
    "    if month is not None:\n",
    "        months_to_analyze = [month]\n",
    "    else:\n",
    "        months_to_analyze = sorted([int(m) for m in results_dict[year].keys()])\n",
    "    \n",
    "    monthly_stats = {}\n",
    "    daily_stats = {}\n",
    "    \n",
    "    # Process statistics for each month\n",
    "    for month in months_to_analyze:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        \n",
    "        if month_str not in results_dict[year]:\n",
    "            print(f\"No results available for {year}-{month_str}\")\n",
    "            continue\n",
    "            \n",
    "        # Count total tracks for this month\n",
    "        total_cs2_tracks = 0\n",
    "        total_is2_tracks = 0\n",
    "        valid_pairs_count = 0\n",
    "        days_in_month = len(results_dict[year][month_str])\n",
    "        \n",
    "        # Get file counts from data dictionary for this month\n",
    "        month_cs2_files = [f for f in data_dict[year]['cs2'] \n",
    "                         if f.endswith('.nc') and f\"_{year}{month_str}\" in os.path.basename(f)]\n",
    "        month_is2_files = [f for f in data_dict[year]['is2']\n",
    "                         if f.endswith('.h5') and f\"_{year}{month_str}\" in os.path.basename(f)]\n",
    "        \n",
    "        total_cs2_tracks = len(month_cs2_files)\n",
    "        total_is2_tracks = len(month_is2_files)\n",
    "        \n",
    "        # Calculate daily statistics\n",
    "        daily_stats[month_str] = {}\n",
    "        \n",
    "        for day, pairs in results_dict[year][month_str].items():\n",
    "            valid_pairs_count += len(pairs)\n",
    "            \n",
    "            # Count CS2 and IS2 files for this specific day\n",
    "            day_cs2_files = [f for f in month_cs2_files \n",
    "                           if f\"_{year}{month_str}{day}\" in os.path.basename(f)]\n",
    "            day_is2_files = [f for f in month_is2_files\n",
    "                           if f\"_{year}{month_str}{day}\" in os.path.basename(f)]\n",
    "            \n",
    "            day_cs2_count = len(day_cs2_files)\n",
    "            day_is2_count = len(day_is2_files)\n",
    "            day_valid_count = len(pairs)\n",
    "            \n",
    "            # Calculate alignment efficiency\n",
    "            if day_cs2_count > 0 and day_is2_count > 0:\n",
    "                # Maximum possible pairs would be min(CS2, IS2) if every track aligned\n",
    "                max_possible_pairs = min(day_cs2_count, day_is2_count)\n",
    "                alignment_ratio = day_valid_count / max_possible_pairs if max_possible_pairs > 0 else 0\n",
    "            else:\n",
    "                alignment_ratio = 0\n",
    "                \n",
    "            daily_stats[month_str][day] = {\n",
    "                'cs2_tracks': day_cs2_count,\n",
    "                'is2_tracks': day_is2_count,\n",
    "                'valid_pairs': day_valid_count,\n",
    "                'alignment_ratio': alignment_ratio\n",
    "            }\n",
    "        \n",
    "        # Calculate monthly alignment efficiency\n",
    "        if total_cs2_tracks > 0 and total_is2_tracks > 0:\n",
    "            max_possible_pairs = min(total_cs2_tracks, total_is2_tracks)\n",
    "            monthly_alignment_ratio = valid_pairs_count / max_possible_pairs if max_possible_pairs > 0 else 0\n",
    "        else:\n",
    "            monthly_alignment_ratio = 0\n",
    "            \n",
    "        monthly_stats[month_str] = {\n",
    "            'cs2_tracks': total_cs2_tracks,\n",
    "            'is2_tracks': total_is2_tracks,\n",
    "            'valid_pairs': valid_pairs_count,\n",
    "            'alignment_ratio': monthly_alignment_ratio,\n",
    "            'days_processed': days_in_month\n",
    "        }\n",
    "    \n",
    "    stats['monthly'] = monthly_stats\n",
    "    stats['daily'] = daily_stats\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def visualize_track_statistics(stats, year, region_name, month=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of track alignment statistics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stats : dict\n",
    "        Dictionary containing track statistics from calculate_track_statistics\n",
    "    year : int\n",
    "        Year of the data\n",
    "    region_name : str\n",
    "        Name of the region ('Ross' or 'Weddell')\n",
    "    month : int or None\n",
    "        Month to visualize (if None, visualize all available months)\n",
    "    \"\"\"\n",
    "    # 1. Monthly alignment ratio plot\n",
    "    if month is None:\n",
    "        # Plot all months\n",
    "        months = sorted(stats['monthly'].keys())\n",
    "        \n",
    "        # Get data for plotting\n",
    "        alignment_ratios = [stats['monthly'][m]['alignment_ratio'] * 100 for m in months]\n",
    "        valid_pairs = [stats['monthly'][m]['valid_pairs'] for m in months]\n",
    "        \n",
    "        # Create figure with two y-axes\n",
    "        fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "        \n",
    "        # First y-axis: Alignment ratio as bars\n",
    "        ax1.bar(months, alignment_ratios, color='steelblue', alpha=0.7, label='Alignment Ratio')\n",
    "        ax1.set_xlabel('Month of ' + str(year))\n",
    "        ax1.set_ylabel('Alignment Ratio (%)', color='steelblue')\n",
    "        ax1.tick_params(axis='y', labelcolor='steelblue')\n",
    "        ax1.set_ylim(0, max(alignment_ratios) * 1.2 if alignment_ratios else 10)\n",
    "        \n",
    "        # Second y-axis: Number of valid pairs as line\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(months, valid_pairs, 'o-', color='darkorange', linewidth=2, label='Valid Pairs')\n",
    "        ax2.set_ylabel('Number of Valid Pairs', color='darkorange')\n",
    "        ax2.tick_params(axis='y', labelcolor='darkorange')\n",
    "        ax2.set_ylim(0, max(valid_pairs) * 1.2 if valid_pairs else 10)\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        plt.title(f'Monthly CRYO2ICE Track Alignment Statistics\\n{region_name} Sea, {year}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Plot daily data for specific month\n",
    "        month_str = f\"{month:02d}\"\n",
    "        if month_str not in stats['daily']:\n",
    "            print(f\"No data available for {year}-{month_str}\")\n",
    "            return\n",
    "            \n",
    "        daily_data = stats['daily'][month_str]\n",
    "        days = sorted(daily_data.keys())\n",
    "        \n",
    "        # Get data for plotting\n",
    "        alignment_ratios = [daily_data[d]['alignment_ratio'] * 100 for d in days]\n",
    "        valid_pairs = [daily_data[d]['valid_pairs'] for d in days]\n",
    "        cs2_counts = [daily_data[d]['cs2_tracks'] for d in days]\n",
    "        is2_counts = [daily_data[d]['is2_tracks'] for d in days]\n",
    "        \n",
    "        # Create a figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), height_ratios=[1, 1.2])\n",
    "        \n",
    "        # First subplot: Alignment ratio\n",
    "        ax1.bar(days, alignment_ratios, color='steelblue')\n",
    "        ax1.set_ylabel('Alignment Ratio (%)')\n",
    "        ax1.set_title(f'Daily CRYO2ICE Track Alignment Ratio - {region_name} Sea, {year}-{month_str}')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add labels to bars\n",
    "        for i, ratio in enumerate(alignment_ratios):\n",
    "            if ratio > 0:\n",
    "                ax1.text(i, ratio + 1, f\"{ratio:.1f}%\", ha='center')\n",
    "        \n",
    "        # Second subplot: Track counts with valid pairs\n",
    "        width = 0.35\n",
    "        x = np.arange(len(days))\n",
    "        \n",
    "        # Plot bars for CS2 and IS2 track counts\n",
    "        ax2.bar(x - width/2, cs2_counts, width, label='CS2 Tracks', color='royalblue', alpha=0.7)\n",
    "        ax2.bar(x + width/2, is2_counts, width, label='IS2 Tracks', color='darkblue', alpha=0.7)\n",
    "        \n",
    "        # Plot valid pairs as a line\n",
    "        ax3 = ax2.twinx()\n",
    "        ax3.plot(x, valid_pairs, 'o-', color='orangered', linewidth=2, label='Valid Pairs')\n",
    "        ax3.set_ylabel('Number of Valid Pairs', color='orangered')\n",
    "        ax3.tick_params(axis='y', labelcolor='orangered')\n",
    "        \n",
    "        # Combine legends from both axes\n",
    "        lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax3.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        ax2.set_xlabel(f'Day of {year}-{month_str}')\n",
    "        ax2.set_ylabel('Number of Tracks')\n",
    "        ax2.set_title(f'Daily Track Counts and Valid Pairs - {region_name} Sea, {year}-{month_str}')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(days)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create a third visualization: Ratio of valid pairs to available data\n",
    "        fig, ax = plt.subplots(figsize=(9, 5))\n",
    "        \n",
    "        # Calculate percentage of CS2 and IS2 tracks that form valid pairs\n",
    "        cs2_utilization = [daily_data[d]['valid_pairs']/daily_data[d]['cs2_tracks']*100 if daily_data[d]['cs2_tracks'] > 0 else 0 for d in days]\n",
    "        is2_utilization = [daily_data[d]['valid_pairs']/daily_data[d]['is2_tracks']*100 if daily_data[d]['is2_tracks'] > 0 else 0 for d in days]\n",
    "        \n",
    "        ax.bar(x - width/2, cs2_utilization, width, label='CS2 Utilization', color='royalblue', alpha=0.7)\n",
    "        ax.bar(x + width/2, is2_utilization, width, label='IS2 Utilization', color='darkblue', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel(f'Day of {year}-{month_str}')\n",
    "        ax.set_ylabel('Percentage of Tracks in Valid Pairs (%)')\n",
    "        ax.set_title(f'Daily Track Utilization - {region_name} Sea, {year}-{month_str}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(days)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96f51d",
   "metadata": {},
   "source": [
    "### 3. Implementation in the Processing Function\n",
    "\n",
    "Let's update the ultra_fast_selection_criteria function to use our oceanographically-informed sampling approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151fdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultra_fast_selection_criteria(cs2_tracks, is2_tracks, distance_threshold=20, time_threshold=3):\n",
    "    \"\"\"\n",
    "    Apply track selection criteria with extreme optimizations for speed using\n",
    "    oceanographically-informed track representations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cs2_tracks : list\n",
    "        List of CS2 track dictionaries\n",
    "    is2_tracks : list\n",
    "        List of IS2 track dictionaries\n",
    "    distance_threshold : float\n",
    "        Maximum distance between tracks in kilometers\n",
    "    time_threshold : float\n",
    "        Maximum time difference between tracks in hours\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list: List of coincident track pairs that meet all criteria\n",
    "    \"\"\"\n",
    "    valid_pairs = []\n",
    "    \n",
    "    print(f\"Evaluating {len(cs2_tracks)} CS2 tracks and {len(is2_tracks)} IS2 tracks...\")\n",
    "    print(f\"Using criteria: distance ≤ {distance_threshold} km, time difference ≤ {time_threshold} hrs\")\n",
    "    \n",
    "    # Create oceanographically-informed track representations\n",
    "    print(\"Creating oceanographically-informed track representations...\")\n",
    "    cs2_optimized = []\n",
    "    is2_optimized = []\n",
    "    \n",
    "    for track in cs2_tracks:\n",
    "        cs2_optimized.append(oceanographic_track_sampling(track, max_points=30))\n",
    "    \n",
    "    for track in is2_tracks:\n",
    "        is2_optimized.append(oceanographic_track_sampling(track, max_points=30))\n",
    "    \n",
    "    # Pre-compute track bounds and time ranges\n",
    "    cs2_bounds = [get_track_bounds(track) for track in cs2_optimized]\n",
    "    is2_bounds = [get_track_bounds(track) for track in is2_optimized]\n",
    "    \n",
    "    # OPTIMIZATION: Implement pre-filtering\n",
    "    print(\"Performing ultra-fast pre-filtering...\")\n",
    "    time_compatible_pairs = []\n",
    "    \n",
    "    for i, cs2_track in enumerate(cs2_optimized):\n",
    "        cs2_time = cs2_track['time'][len(cs2_track['time']) // 2]  # Use middle time\n",
    "        \n",
    "        for j, is2_track in enumerate(is2_optimized):\n",
    "            is2_time = is2_track['time'][len(is2_track['time']) // 2]\n",
    "            time_diff_hours = abs((cs2_time - is2_time).total_seconds()) / 3600\n",
    "            \n",
    "            # Filter by time first\n",
    "            if time_diff_hours <= time_threshold:\n",
    "                # Then check bounds distance\n",
    "                bounds_dist = bounds_distance(cs2_bounds[i], is2_bounds[j])\n",
    "                \n",
    "                if bounds_dist <= distance_threshold:\n",
    "                    time_compatible_pairs.append((i, j, bounds_dist))\n",
    "    \n",
    "    print(f\"Found {len(time_compatible_pairs)} potential pairs after pre-filtering\")\n",
    "    \n",
    "    # Sort by increasing bounds distance for efficiency\n",
    "    time_compatible_pairs.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # OPTIMIZATION: Vectorized minimum distance calculation\n",
    "    print(\"Evaluating precise distances...\")\n",
    "    for i, j, _ in tqdm(time_compatible_pairs, desc=\"Processing track pairs\"):\n",
    "        # Use vectorized minimum distance calculation\n",
    "        min_dist, _ = vectorized_track_distance(cs2_optimized[i], is2_optimized[j])\n",
    "        \n",
    "        if min_dist <= distance_threshold:\n",
    "            # All criteria met - we're using optimized tracks for fast calculation\n",
    "            # and assuming intersection time requirement is met when distance is small enough\n",
    "            valid_pairs.append({\n",
    "                'cs2_track': cs2_tracks[i],  # Store original track info\n",
    "                'is2_track': is2_tracks[j],  # Store original track info\n",
    "                'min_distance': min_dist,\n",
    "                'time_difference': abs((cs2_optimized[i]['time'][0] - is2_optimized[j]['time'][0]).total_seconds()) / 3600,\n",
    "                'intersection_time': 1.5  # Default conservative estimate\n",
    "            })\n",
    "            print(f\"  Found valid pair {len(valid_pairs)}: dist={min_dist:.2f}km\")\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid CRYO2ICE coincident tracks\")\n",
    "    return valid_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074ea30",
   "metadata": {},
   "source": [
    "### 4. Using the New Methods\n",
    "\n",
    "Let's update the process_ross_sea_month function to use our new statistical analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db213e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ross_sea_month(data_dict, year, month):\n",
    "    \"\"\"\n",
    "    Process Ross Sea data for an entire month with enhanced statistical analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Dictionary containing file paths\n",
    "    year : int\n",
    "        Year to process\n",
    "    month : int\n",
    "        Month to process\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary with valid track pairs organized by day\n",
    "    \"\"\"\n",
    "    # Determine number of days in the month\n",
    "    if month in [4, 6, 9, 11]:\n",
    "        days_in_month = 30\n",
    "    elif month == 2:\n",
    "        days_in_month = 29 if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)) else 28\n",
    "    else:\n",
    "        days_in_month = 31\n",
    "    \n",
    "    # Dictionary to store valid pairs by date\n",
    "    valid_pairs_by_date = {}\n",
    "    month_str = f\"{month:02d}\"\n",
    "    \n",
    "    print(f\"\\nProcessing Ross Sea data for {year}-{month_str} ({days_in_month} days)...\")\n",
    "    \n",
    "    # Process each day in the month\n",
    "    monthly_pair_count = 0\n",
    "    for day in range(1, days_in_month + 1):\n",
    "        day_str = f\"{day:02d}\"\n",
    "        \n",
    "        # Use our faster processing function with oceanographic sampling\n",
    "        day_valid_pairs = process_region_single_day_fast(data_dict, \"Ross\", year, month, day)\n",
    "        \n",
    "        # Store the results\n",
    "        valid_pairs_by_date[day_str] = day_valid_pairs\n",
    "        monthly_pair_count += len(day_valid_pairs)\n",
    "        \n",
    "        print(f\"Day {month_str}-{day_str}: Found {len(day_valid_pairs)} valid track pairs\")\n",
    "    \n",
    "    print(f\"\\nMonth {month_str} summary: Found {monthly_pair_count} valid track pairs\")\n",
    "    \n",
    "    # Calculate and visualize track statistics\n",
    "    print(\"\\nCalculating track alignment statistics...\")\n",
    "    month_results = {year: {month_str: valid_pairs_by_date}}\n",
    "    stats = calculate_track_statistics(month_results, data_dict, year, month)\n",
    "    visualize_track_statistics(stats, year, \"Ross\", month)\n",
    "    \n",
    "    return valid_pairs_by_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a707b52",
   "metadata": {},
   "source": [
    "### Let's execute this with a test month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7388b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store Ross Sea results for 2024\n",
    "ross_results = {2024: {}}  # Initialize the structure for 2024\n",
    "\n",
    "# Define the specific year and months to process\n",
    "year = 2024\n",
    "months_to_process = [8, 9, 10]  # August through October\n",
    "\n",
    "# Track valid pairs for summary\n",
    "total_valid_pairs = 0\n",
    "monthly_counts = {2024: {}}\n",
    "\n",
    "print(\"Starting CRYO2ICE track alignment analysis for August-October 2024\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Process each month\n",
    "for month in months_to_process:\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Processing {year}-{month:02d}\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    \n",
    "    # Execute the enhanced processing with oceanographic track sampling\n",
    "    ross_results[year][f\"{month:02d}\"] = process_ross_sea_month(ross_data, year, month)\n",
    "    \n",
    "    # Calculate monthly totals\n",
    "    month_str = f\"{month:02d}\"\n",
    "    monthly_total = sum(len(ross_results[year][month_str][day]) for day in ross_results[year][month_str])\n",
    "    monthly_counts[year][month] = monthly_total\n",
    "    total_valid_pairs += monthly_total\n",
    "    \n",
    "    print(f\"\\n{year}-{month:02d} summary: Found {monthly_total} valid CRYO2ICE coincident tracks\")\n",
    "\n",
    "# Print overall summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total valid CRYO2ICE coincident tracks for August-October 2024: {total_valid_pairs}\")\n",
    "\n",
    "# Display monthly breakdown\n",
    "print(\"\\nMonthly breakdown:\")\n",
    "for month in months_to_process:\n",
    "    month_name = {8: \"August\", 9: \"September\", 10: \"October\"}[month]\n",
    "    print(f\"  {month_name} 2024: {monthly_counts[year][month]} valid track pairs\")\n",
    "\n",
    "# Calculate and visualize overall statistics across all processed months\n",
    "print(\"\\nCalculating and visualizing overall track statistics...\")\n",
    "all_months_results = {year: {}}\n",
    "for month in months_to_process:\n",
    "    month_str = f\"{month:02d}\"\n",
    "    all_months_results[year][month_str] = ross_results[year][month_str]\n",
    "\n",
    "stats = calculate_track_statistics(all_months_results, ross_data, year)\n",
    "visualize_track_statistics(stats, year, \"Ross\")\n",
    "\n",
    "# Create summary table for all months\n",
    "summary_df = pd.DataFrame(columns=['Month', 'CS2 Tracks', 'IS2 Tracks', 'Valid Pairs', 'Alignment Ratio (%)'])\n",
    "row = 0\n",
    "\n",
    "for month in months_to_process:\n",
    "    month_str = f\"{month:02d}\"\n",
    "    month_name = {8: \"August\", 9: \"September\", 10: \"October\"}[month]\n",
    "    \n",
    "    if month_str in stats['monthly']:\n",
    "        summary_df.loc[row] = [\n",
    "            month_name,\n",
    "            stats['monthly'][month_str]['cs2_tracks'],\n",
    "            stats['monthly'][month_str]['is2_tracks'],\n",
    "            stats['monthly'][month_str]['valid_pairs'],\n",
    "            stats['monthly'][month_str]['alignment_ratio'] * 100\n",
    "        ]\n",
    "        row += 1\n",
    "\n",
    "print(\"\\nMonthly statistics summary for August-October 2024:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Generate detailed visualizations for each month individually\n",
    "print(\"\\nGenerating detailed visualizations for each month...\")\n",
    "for month in months_to_process:\n",
    "    month_str = f\"{month:02d}\"\n",
    "    month_name = {8: \"August\", 9: \"September\", 10: \"October\"}[month]\n",
    "    \n",
    "    print(f\"\\nDetailed statistics for {month_name} 2024:\")\n",
    "    month_results = {year: {month_str: ross_results[year][month_str]}}\n",
    "    month_stats = calculate_track_statistics(month_results, ross_data, year, month)\n",
    "    \n",
    "    # Display daily statistics in a table format\n",
    "    days = sorted(month_stats['daily'][month_str].keys())\n",
    "    daily_stats_df = pd.DataFrame(index=days, columns=['CS2 Tracks', 'IS2 Tracks', 'Valid Pairs', 'Alignment Ratio (%)'])\n",
    "    \n",
    "    for day in days:\n",
    "        day_stats = month_stats['daily'][month_str][day]\n",
    "        daily_stats_df.loc[day, 'CS2 Tracks'] = day_stats['cs2_tracks']\n",
    "        daily_stats_df.loc[day, 'IS2 Tracks'] = day_stats['is2_tracks']\n",
    "        daily_stats_df.loc[day, 'Valid Pairs'] = day_stats['valid_pairs']\n",
    "        daily_stats_df.loc[day, 'Alignment Ratio (%)'] = day_stats['alignment_ratio'] * 100\n",
    "    \n",
    "    print(f\"Daily statistics for {month_name} 2024:\")\n",
    "    print(daily_stats_df)\n",
    "    \n",
    "    # Create visualizations for this month\n",
    "    visualize_track_statistics(month_stats, year, \"Ross\", month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
